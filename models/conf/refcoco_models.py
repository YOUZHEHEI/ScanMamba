"""
cobra/conf/refcoco_models.py

ä¿®å¾©å¾ªç’°å°å…¥å•é¡Œçš„RefCOCOæ¨¡å‹é…ç½®
"""
from dataclasses import dataclass
from typing import Optional, Dict, Any

# é¿å…å¾ªç’°å°å…¥ - åœ¨éœ€è¦æ™‚å‹•æ…‹å°å…¥
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from cobra.conf.models import ModelConfig


@dataclass 
class BaseRefCOCOConfig:
    """RefCOCOåŸºç¤é…ç½®é¡ï¼Œé¿å…å¾ªç’°å°å…¥"""
    
    # Model identification
    model_id: str = "cobra-spatial-refcoco+3b"
    arch_specifier: str = "no-align+fused-gelu-mlp"
    
    # Backbone configuration
    vision_backbone_id: str = "dinosiglip-vit-so-384px"
    llm_backbone_id: str = "mamba-2.8b-zephyr"
    
    # Model parameters
    image_resize_strategy: str = "resize-naive"
    llm_max_length: int = 1024
    
    # Spatial reasoning configuration
    enable_spatial_reasoning: bool = True
    spatial_reasoning_config: Optional[Dict[str, Any]] = None
    
    # Training stages - Align stage (skip for RefCOCO)
    align_epochs: int = 0
    align_global_batch_size: int = 0
    align_per_device_batch_size: int = 0
    align_learning_rate: float = 0.0
    align_weight_decay: float = 0.0
    align_max_grad_norm: float = 0.0
    align_lr_scheduler_type: str = "linear-warmup+cosine-decay"
    align_warmup_ratio: float = 0.0
    align_train_strategy: str = "single-gpu"
    
    # Finetune stage
    finetune_epochs: int = 3
    finetune_global_batch_size: int = 16
    finetune_per_device_batch_size: int = 2
    finetune_learning_rate: float = 1e-4
    finetune_weight_decay: float = 0.01
    finetune_max_grad_norm: float = 1.0
    finetune_lr_scheduler_type: str = "linear-warmup+cosine-decay"
    finetune_warmup_ratio: float = 0.1
    finetune_train_strategy: str = "single-gpu"
    
    # LoRA configuration
    lora_rank: int = 16
    lora_alpha: float = 32.0
    lora_dropout: float = 0.1
    
    # LoRA training parameters
    lora_finetune_epochs: int = 5
    lora_finetune_global_batch_size: int = 8
    lora_finetune_per_device_batch_size: int = 1
    lora_finetune_learning_rate: float = 2e-4
    lora_finetune_weight_decay: float = 0.01
    lora_finetune_max_grad_norm: float = 1.0
    lora_finetune_lr_scheduler_type: str = "linear-warmup+cosine-decay"
    lora_finetune_warmup_ratio: float = 0.1
    lora_finetune_train_strategy: str = "single-gpu"
    
    def __post_init__(self):
        # Set default spatial reasoning config
        if self.spatial_reasoning_config is None:
            self.spatial_reasoning_config = {
                "d_state": 16,
                "d_conv": 4,
                "expand": 2,
                "dropout": 0.1,
                "num_directions": 4,
                "use_bias": False,
            }


@dataclass
class CobraSpatialRefCOCOConfig(BaseRefCOCOConfig):
    """æ¨™æº–RefCOCOé…ç½®"""
    model_id: str = "cobra-spatial-refcoco+3b"


@dataclass
class CobraSpatialRefCOCOLoRAConfig(BaseRefCOCOConfig):
    """LoRAå°ˆç”¨RefCOCOé…ç½®"""
    
    model_id: str = "cobra-spatial-refcoco-lora+3b"
    
    # è·³éæ¨™æº–å¾®èª¿éšæ®µ
    finetune_epochs: int = 0
    finetune_global_batch_size: int = 0
    finetune_per_device_batch_size: int = 0
    finetune_learning_rate: float = 0.0
    
    # å¢å¼·LoRAè¨“ç·´
    lora_rank: int = 32
    lora_alpha: float = 64.0
    lora_dropout: float = 0.05
    
    lora_finetune_epochs: int = 8
    lora_finetune_global_batch_size: int = 16
    lora_finetune_per_device_batch_size: int = 2
    lora_finetune_learning_rate: float = 3e-4


@dataclass
class CobraSpatialRefCOCOLightConfig(BaseRefCOCOConfig):
    """è¼•é‡ç´šRefCOCOé…ç½®"""
    
    model_id: str = "cobra-spatial-refcoco-light+3b"
    
    # ä½¿ç”¨æ›´å°çš„è¦–è¦ºbackbone
    vision_backbone_id: str = "siglip-vit-so400m"
    
    # æ¸›å°‘åºåˆ—é•·åº¦
    llm_max_length: int = 512
    
    # è¨˜æ†¶é«”å„ªåŒ–è¨“ç·´è¨­ç½®
    finetune_epochs: int = 2
    finetune_global_batch_size: int = 4
    finetune_per_device_batch_size: int = 1
    finetune_learning_rate: float = 5e-5
    
    # ä¿å®ˆçš„LoRAè¨­ç½®
    lora_rank: int = 8
    lora_alpha: float = 16.0
    lora_dropout: float = 0.1
    
    lora_finetune_epochs: int = 3
    lora_finetune_global_batch_size: int = 4
    lora_finetune_per_device_batch_size: int = 1
    lora_finetune_learning_rate: float = 1e-4
    
    def __post_init__(self):
        super().__post_init__()
        # ä½¿ç”¨æ›´è¼•é‡çš„ç©ºé–“é…ç½®
        self.spatial_reasoning_config = {
            "d_state": 8,
            "d_conv": 3,
            "expand": 1,
            "dropout": 0.1,
            "num_directions": 3,
            "use_bias": False,
        }


# åœ¨ cobra/conf/refcoco_models.py ä¸­æ·»åŠ 6æ–¹å‘é…ç½®ç±»

@dataclass
class Cobra6DirRefCOCOLoRAConfig(BaseRefCOCOConfig):
    """6æ–¹å‘RefCOCO LoRAé…ç½®"""
    
    model_id: str = "cobra-6dir-refcoco-lora+3b"
    
    # ç©ºé—´æ¨ç†é…ç½®
    enable_spatial_reasoning: bool = True
    spatial_reasoning_config: Optional[Dict[str, Any]] = None
    num_scan_directions: int = 6  # 6ä¸ªæ‰«ææ–¹å‘
    enable_semantic_alignment: bool = True  # å¯ç”¨è¯­ä¹‰å¯¹é½
    
    # è·³è¿‡æ ‡å‡†å¾®è°ƒé˜¶æ®µ
    finetune_epochs: int = 0
    finetune_global_batch_size: int = 0
    finetune_per_device_batch_size: int = 0
    finetune_learning_rate: float = 0.0
    
    # å¢å¼ºLoRAè®­ç»ƒé…ç½®
    lora_rank: int = 32
    lora_alpha: float = 64.0
    lora_dropout: float = 0.05
    lora_target_modules_str: str = "mixer.in_proj,mixer.out_proj,mixer.x_proj,mixer.dt_proj,spatial_scanner.direction_projections,spatial_scanner.fusion_layer"
    
    lora_finetune_epochs: int = 3
    lora_finetune_global_batch_size: int = 8
    lora_finetune_per_device_batch_size: int = 1
    lora_finetune_learning_rate: float = 2e-4
    lora_finetune_weight_decay: float = 0.01
    lora_finetune_max_grad_norm: float = 1.0
    lora_finetune_lr_scheduler_type: str = "linear-warmup+cosine-decay"
    lora_finetune_warmup_ratio: float = 0.1
    lora_finetune_train_strategy: str = "single-gpu"
    
    def __post_init__(self):
        # è®¾ç½®é»˜è®¤6æ–¹å‘ç©ºé—´æ¨ç†é…ç½®
        if self.spatial_reasoning_config is None:
            self.spatial_reasoning_config = {
                "d_state": 16,
                "d_conv": 4,
                "expand": 2,
                "dropout": 0.1,
                "num_directions": 6,  # 6ä¸ªæ–¹å‘ï¼šleft-right, right-left, up-down, down-up, transpose, transpose-reverse
                "use_bias": False,
                "enable_semantic_alignment": True,
                "text_embed_dim": None,  # å°†è‡ªåŠ¨åŒ¹é…LLMåµŒå…¥ç»´åº¦
            }
        
        # è§£æLoRAç›®æ ‡æ¨¡å—
        super().__post_init__()


@dataclass  
class Cobra6DirRefCOCOConfig(BaseRefCOCOConfig):
    """6æ–¹å‘RefCOCOå®Œæ•´è®­ç»ƒé…ç½®"""
    
    model_id: str = "cobra-6dir-spatial-refcoco+3b"
    
    # ç©ºé—´æ¨ç†é…ç½®
    enable_spatial_reasoning: bool = True
    spatial_reasoning_config: Optional[Dict[str, Any]] = None
    num_scan_directions: int = 6
    enable_semantic_alignment: bool = True
    
    # æ ‡å‡†å¾®è°ƒé˜¶æ®µ
    finetune_epochs: int = 2
    finetune_global_batch_size: int = 8
    finetune_per_device_batch_size: int = 1
    finetune_learning_rate: float = 1e-4
    
    # LoRAå¾®è°ƒé˜¶æ®µ
    lora_rank: int = 32
    lora_alpha: float = 64.0
    lora_dropout: float = 0.05
    
    lora_finetune_epochs: int = 3
    lora_finetune_global_batch_size: int = 8
    lora_finetune_per_device_batch_size: int = 1
    lora_finetune_learning_rate: float = 2e-4
    
    def __post_init__(self):
        # è®¾ç½®6æ–¹å‘ç©ºé—´æ¨ç†é…ç½®
        if self.spatial_reasoning_config is None:
            self.spatial_reasoning_config = {
                "d_state": 16,
                "d_conv": 4,
                "expand": 2,
                "dropout": 0.1,
                "num_directions": 6,
                "use_bias": False,
                "enable_semantic_alignment": True,
            }
        
        super().__post_init__()


@dataclass
class Cobra6DirRefCOCOLightConfig(BaseRefCOCOConfig):
    """6æ–¹å‘RefCOCOè½»é‡çº§é…ç½®"""
    
    model_id: str = "cobra-6dir-refcoco-light+3b"
    
    # ä½¿ç”¨è¾ƒå°çš„è§†è§‰backbone
    vision_backbone_id: str = "siglip-vit-so400m"
    
    # ç©ºé—´æ¨ç†é…ç½®ï¼ˆè½»é‡çº§ï¼‰
    enable_spatial_reasoning: bool = True
    spatial_reasoning_config: Optional[Dict[str, Any]] = None
    num_scan_directions: int = 6
    enable_semantic_alignment: bool = True
    
    # å‡å°‘åºåˆ—é•¿åº¦
    llm_max_length: int = 256
    
    # å†…å­˜ä¼˜åŒ–è®­ç»ƒè®¾ç½®
    finetune_epochs: int = 1
    finetune_global_batch_size: int = 4
    finetune_per_device_batch_size: int = 1
    finetune_learning_rate: float = 1e-4
    
    lora_rank: int = 16  # æ›´å°çš„rank
    lora_alpha: float = 32.0
    lora_dropout: float = 0.1
    
    lora_finetune_epochs: int = 2
    lora_finetune_global_batch_size: int = 4
    lora_finetune_per_device_batch_size: int = 1
    lora_finetune_learning_rate: float = 1e-4
    
    def __post_init__(self):
        # è½»é‡çº§ç©ºé—´æ¨ç†é…ç½®
        if self.spatial_reasoning_config is None:
            self.spatial_reasoning_config = {
                "d_state": 8,  # æ›´å°çš„çŠ¶æ€ç»´åº¦
                "d_conv": 3,
                "expand": 1,  # æ›´å°çš„æ‰©å±•å› å­
                "dropout": 0.1,
                "num_directions": 6,
                "use_bias": False,
                "enable_semantic_alignment": True,
            }
        
        super().__post_init__()


# å‹•æ…‹å‰µå»ºModelConfigå­é¡ä»¥é¿å…å¾ªç’°å°å…¥
def create_model_configs():
    """å‹•æ…‹å‰µå»ºæ¨¡å‹é…ç½®é¡ä»¥é¿å…å¾ªç’°å°å…¥"""
    try:
        from cobra.conf.models import ModelConfig
        
        # å‰µå»ºç¹¼æ‰¿ModelConfigçš„é¡
        class CobraSpatialRefCOCOModelConfig(ModelConfig, CobraSpatialRefCOCOConfig):
            pass
        
        class CobraSpatialRefCOCOLoRAModelConfig(ModelConfig, CobraSpatialRefCOCOLoRAConfig):
            pass
            
        class CobraSpatialRefCOCOLightModelConfig(ModelConfig, CobraSpatialRefCOCOLightConfig):
            pass
        
        return {
            'standard': CobraSpatialRefCOCOModelConfig,
            'lora': CobraSpatialRefCOCOLoRAModelConfig,
            'light': CobraSpatialRefCOCOLightModelConfig,
        }
    except ImportError:
        # å¦‚æœä»æœ‰å¾ªç’°å°å…¥å•é¡Œï¼Œè¿”å›åŸºç¤é…ç½®
        return {
            'standard': CobraSpatialRefCOCOConfig,
            'lora': CobraSpatialRefCOCOLoRAConfig,
            'light': CobraSpatialRefCOCOLightConfig,
        }

@dataclass
class Cobra6DirRefCOCO8BLoRAConfig(BaseRefCOCOConfig):
    """6æ–¹å‘RefCOCO LoRAé…ç½® - Mamba 8Bç‰ˆæœ¬"""
    
    model_id: str = "cobra-6dir-refcoco-lora+8b"
    llm_backbone_id: str = "mamba-8b"  # ğŸ‘ˆ ä½¿ç”¨ Mamba-8B
    
    # ç©ºé–“æ¨ç†é…ç½®
    enable_spatial_reasoning: bool = True
    num_scan_directions: int = 6
    enable_semantic_alignment: bool = True
    
    # LoRAé…ç½® - 8Bæ¨¡å‹å„ªåŒ–
    lora_rank: int = 64
    lora_alpha: float = 128.0
    lora_dropout: float = 0.05
    lora_target_modules_str: str = "mixer.in_proj,mixer.out_proj,mixer.x_proj,mixer.dt_proj,spatial_scanner.direction_projections,spatial_scanner.fusion_layer"
    
    # è¨“ç·´é…ç½®
    lora_finetune_epochs: int = 3
    lora_finetune_global_batch_size: int = 8
    lora_finetune_per_device_batch_size: int = 1
    lora_finetune_learning_rate: float = 1e-4
    lora_finetune_weight_decay: float = 0.01
    
    # è¨˜æ†¶é«”å„ªåŒ–
    gradient_accumulation_steps: int = 32
    enable_gradient_checkpointing: bool = True
    
    def __post_init__(self):
        if self.spatial_reasoning_config is None:
            self.spatial_reasoning_config = {
                "d_state": 16,
                "d_conv": 4,
                "expand": 2,
                "dropout": 0.1,
                "num_directions": 6,
                "use_bias": False,
                "enable_semantic_alignment": True,
            }
        super().__post_init__()

# å»¶é²åˆå§‹åŒ–
_model_configs = None

def get_refcoco_config(config_type: str = 'lora'):
    """ç²å–RefCOCOé…ç½®ï¼Œé¿å…å¾ªç’°å°å…¥"""
    global _model_configs
    if _model_configs is None:
        _model_configs = create_model_configs()
    
    return _model_configs.get(config_type, _model_configs['lora'])